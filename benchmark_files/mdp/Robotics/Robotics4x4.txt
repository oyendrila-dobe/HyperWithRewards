mdp

const int n = 3;

module secret_thread1

	x : [0..3]; 
	y : [0..3];
	r : [0..1];

	[] (x=0)&(y=0)			-> 9/10: (x'=1) + 1/10: (x'=0);
	[] (x=0)&(y=0)			-> 9/10: (y'=1) + 1/10: (y'=0);

	[] (x=1)&(y=0)			-> 9/10: (x'=0) + 1/10: (x'=1);
	[] (x=1)&(y=0)			-> 9/10: (y'=1) + 1/10: (y'=0);

	[] (x=2)&(y=0)			-> 8/10: (x'=1) + 2/10: (x'=2);
	[] (x=2)&(y=0)			-> 9/10: (y'=1) + 1/10: (y'=0);

	[] (x=3)&(y=0)			-> 9/10: (y'=1) + 1/10: (y'=0);

	[] (x=0)&(y=1)			-> 9/10: (x'=1) + 1/10: (x'=0);
	[] (x=0)&(y=1)			-> 9/10: (y'=0) + 1/10: (y'=1);

	[] (x=1)&(y=1)			-> 9/10: (x'=0) + 1/10: (x'=1);
	[] (x=1)&(y=1)			-> 9/10: (y'=2) + 1/10: (y'=1);
	[] (x=1)&(y=1)			-> 9/10: (x'=2) + 1/10: (x'=1);
	[] (x=1)&(y=1)			-> 9/10: (y'=0) + 1/10: (y'=1);

	[] (x=2)&(y=1)			-> 8/10: (x'=1) + 2/10: (x'=3);
	[] (x=2)&(y=1)			-> 9/10: (y'=0) + 1/10: (x'=3);
	[] (x=2)&(y=1)			-> (x'=3);

   	[] (x=3)&(y=1)			-> 9/10: (x'=2) + 1/10: (x'=3);
	[] (x=3)&(y=1)			-> 9/10: (y'=0) + 1/10: (y'=1);

	[] (x=0)&(y=2)			-> 7/10: (x'=1) + 2/10: (x'=0) + 1/10: (y'=3);
	[] (x=0)&(y=2)			-> 7/10: (y'=3) + 2/10: (x'=0) + 1/10: (x'=1);

	[] (x=1)&(y=2)			-> true;

	[] (x=2)&(y=2)			-> 7/10: (x'=3) + 2/10: (x'=2) + 1/10: (y'=3);
	[] (x=2)&(y=2)			-> 7/10: (y'=3) + 2/10: (x'=2) + 1/10: (x'=3);

	[] (x=3)&(y=2)			-> 7/10: (x'=2) + 2/10: (x'=3) + 1/10: (y'=3);
	[] (x=3)&(y=2)			-> 7/10: (y'=3) + 2/10: (x'=3) + 1/10: (x'=2);

	[] (x=0)&(y=3)			-> 7/10: (x'=1) + 2/10: (x'=0) + 1/10: (y'=2);
	[] (x=0)&(y=3)			-> 7/10: (y'=2) + 2/10: (x'=0) + 1/10: (x'=1);

	[] (x=1)&(y=3)			-> 8/10: (x'=0) + 2/10: (x'=1);
	[] (x=1)&(y=3)			-> 8/10: (x'=2) + 2/10: (x'=1);

	[] (x=2)&(y=3)			-> 7/10: (x'=1) + 2/10: (x'=2) + 1/10: (y'=2);
	[] (x=2)&(y=3)			-> 7/10: (x'=3) + 2/10: (x'=2) + 1/10: (y'=2);
	[] (x=2)&(y=3)			-> 6/10: (y'=2) + 2/10: (x'=2) + 1/10: (x'=1) + 1/10: (x'=3);

	[] (x=3)&(y=3)			-> 7/10: (x'=2) + 2/10: (x'=3) + 1/10: (y'=2);
	[] (x=3)&(y=3)			-> 7/10: (y'=2) + 2/10: (x'=3) + 1/10: (x'=2);

endmodule

//for every scheduler of robot 1, every scheduler of robot 2 has a better reward (and both probabilities are 1)
//more energy/higher reward for rougher terrain
//action=choice of direction, low probability to go into another direction or stay in current location

//python source.py benchmark_files/mdp/model.nm "AS sh . A s1 . A s2 . ~(((start0(s1) & start1(s2)) & ((P (F end(s1)) = 1) & (P (F end(s2)) = 1))) & ~(R s1 (F end(s1)) < R s2 (F end(s2))))"

init true endinit

label "start0" = (x=3)&(y=0)&(r=0);
label "start1" = (x=3)&(y=3)&(r=1);

label "end" = (x=1)&(y=2);

rewards
    (x=3)&(y<2) : 1;
    (x<2)&(y<2) : 1;
    (x=2)&(y<2) : 2;
    (y=3)       : 3;
    (x=0)&(y=2) : 3;
    (x>1)&(y=2) : 3;
    (x=1)&(y=2) : 0;
endrewards