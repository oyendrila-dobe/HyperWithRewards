mdp

const int n = 2;

module secret_thread1

	x : [0..2]; 
	y : [0..2];
	r : [0..1];

	[] (x=0)&(y=0)			-> 9/10: (x'=1) + 1/10: (x'=0);
	[] (x=0)&(y=0)			-> 9/10: (y'=1) + 1/10: (y'=0);

	[] (x=1)&(y=0)			-> 9/10: (x'=0) + 1/10: (x'=1);
	[] (x=1)&(y=0)			-> 9/10: (y'=1) + 1/10: (y'=0);

	[] (x=2)&(y=0)			-> 9/10: (y'=1) + 1/10: (y'=0);

	[] (x=0)&(y=1)			-> 9/10: (x'=1) + 1/10: (x'=0);
	[] (x=0)&(y=1)			-> 9/10: (y'=2) + 1/10: (y'=1);
	[] (x=0)&(y=1)			-> 9/10: (y'=0) + 1/10: (y'=1);

	[] (x=1)&(y=1)			-> 9/10: (x'=0) + 1/10: (x'=1);
	[] (x=1)&(y=1)			-> 9/10: (x'=2) + 1/10: (x'=1);
	[] (x=1)&(y=1)			-> 9/10: (y'=0) + 1/10: (y'=1);

	[] (x=2)&(y=1)			-> 9/10: (x'=1) + 1/10: (x'=2);
	[] (x=2)&(y=1)			-> 9/10: (y'=0) + 1/10: (y'=1);

	[] (x=0)&(y=2)			-> true;

	[] (x=1)&(y=2)			-> 9/10: (x'=0) + 1/10: (x'=1);
	[] (x=1)&(y=2)			-> 9/10: (x'=2) + 1/10: (x'=1);

	[] (x=2)&(y=2)			-> 9/10: (x'=1) + 1/10: (x'=2);


endmodule

//for every scheduler of robot 1, every scheduler of robot 2 has a better reward (and both probabilities are 1)
//more energy/higher reward for rougher terrain
//action=choice of direction, low probability to go into another direction or stay in current location

//python source.py benchmark_files/mdp/model.nm "AS sh . A s1 . A s2 . ~(((start0(s1) & start1(s2)) & ((P (F end(s1)) = 1) & (P (F end(s2)) = 1))) & ~(R s1 (F end(s1)) < R s2 (F end(s2))))"

init true endinit

label "start0" = (x=2)&(y=0)&(r=0);
label "start1" = (x=2)&(y=2)&(r=1);

label "end" = (x=0)&(y=2);

rewards
    true : 1;
endrewards