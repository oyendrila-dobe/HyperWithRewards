mdp

const int n = 3;

module secret_thread1

	x : [0..6]; 
	y : [0..6];
	r : [0..1];

	[] (x=0)&(y=0)			-> 7/10: (y'=1) + 2/10: (y'=0) + 1/10: (x'=1);
	[] (x=0)&(y=0)			-> 7/10: (x'=1) + 2/10: (y'=0) + 1/10: (y'=1);

	[] (x=6)&(y=0)			-> 7/10: (y'=1) + 2/10: (y'=0) + 1/10: (x'=5);
	[] (x=6)&(y=0)			-> 7/10: (x'=5) + 2/10: (y'=0) + 1/10: (y'=1);

	[] (x=0)&(y=6)			-> 7/10: (y'=5) + 2/10: (y'=6) + 1/10: (x'=1);
	[] (x=0)&(y=6)			-> 7/10: (x'=1) + 2/10: (y'=6) + 1/10: (y'=5);

	[] (x=6)&(y=6)			-> 7/10: (y'=5) + 2/10: (y'=6) + 1/10: (x'=5);
	[] (x=6)&(y=6)			-> 7/10: (x'=5) + 2/10: (y'=6) + 1/10: (y'=5);

	[] (x>0)&(x<6)&(y=0)		-> 7/10: (x'=x+1) + 2/10: (x'=x) + 1/10: (y'=1);
	[] (x>0)&(x<6)&(y=0)		-> 7/10: (x'=x-1) + 2/10: (x'=x) + 1/10: (y'=1);
	[] (x>0)&(x<6)&(y=0)		-> 6/10: (y'=1) + 2/10: (y'=0) + 1/10: (x'=x+1) + 1/10: (x'=x-1);

	[] (x>0)&(x<6)&(y=6)		-> 7/10: (x'=x+1) + 2/10: (x'=x) + 1/10: (y'=5);
	[] (x>0)&(x<6)&(y=6)		-> 7/10: (x'=x-1) + 2/10: (x'=x) + 1/10: (y'=5);
	[] (x>0)&(x<6)&(y=6)		-> 6/10: (y'=5) + 2/10: (y'=6) + 1/10: (x'=x+1) + 1/10: (x'=x-1);

	[] (y>0)&(y<6)&(x=0)		-> 7/10: (y'=y+1) + 2/10: (y'=y) + 1/10: (x'=1);
	[] (y>0)&(y<6)&(x=0)		-> 7/10: (y'=y-1) + 2/10: (y'=y) + 1/10: (x'=1);
	[] (y>0)&(y<6)&(x=0)		-> 6/10: (x'=1) + 2/10: (x'=0) + 1/10: (y'=y+1) + 1/10: (y'=y-1);

	[] (y>0)&(y<6)&(x=6)		-> 7/10: (y'=y+1) + 2/10: (y'=y) + 1/10: (x'=5);
	[] (y>0)&(y<6)&(x=6)		-> 7/10: (y'=y-1) + 2/10: (y'=y) + 1/10: (x'=5);
	[] (y>0)&(y<6)&(x=6)		-> 6/10: (x'=5) + 2/10: (x'=6) + 1/10: (y'=y+1) + 1/10: (y'=y-1);

	[] (x>0)&(x<6)&(y>0)&(y<6)	-> 6/10: (x'=x+1) + 2/10: (x'=x) + 1/10: (y'=y+1) + 1/10: (y'=y-1);
	[] (x>0)&(x<6)&(y>0)&(y<6)	-> 6/10: (x'=x-1) + 2/10: (x'=x) + 1/10: (y'=y+1) + 1/10: (y'=y-1);
	[] (x>0)&(x<6)&(y>0)&(y<6)	-> 6/10: (y'=y+1) + 2/10: (y'=y) + 1/10: (x'=x+1) + 1/10: (x'=x-1);
	[] (x>0)&(x<6)&(y>0)&(y<6)	-> 6/10: (y'=y-1) + 2/10: (y'=y) + 1/10: (x'=x+1) + 1/10: (x'=x-1);

endmodule

//for every scheduler of robot 1, every scheduler of robot 2 has a better reward (and both probabilities are 1)
//more energy/higher reward for rougher terrain
//action=choice of direction, low probability to go into another direction or stay in current location

//python source.py benchmark_files/mdp/model.nm "AS sh . A s1 . A s2 . ~(((start0(s1) & start1(s2)) & ((P (F end(s1)) = 1) & (P (F end(s2)) = 1))) & ~(R s1 (F end(s1)) < R s2 (F end(s2))))"

init true endinit

label "start0" = (x=0)&(y=0)&(r=0);
label "start1" = (x=6)&(y=6)&(r=1);

label "end" = (x=3)&(y=3);

rewards
    true : 3;
endrewards